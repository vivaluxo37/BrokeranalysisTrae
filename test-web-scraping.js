import WebScrapingService from './src/services/webScrapingService.js';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config();

// Initialize the web scraping service
const scraper = new WebScrapingService();

async function testWebScraping() {
  console.log('ðŸ§ª Testing Web Scraping Service...');
  console.log('=' .repeat(50));

  try {
    // Test 1: Single page scraping
    console.log('\n1ï¸âƒ£ Testing single page scraping...');
    
    const testUrl = 'https://httpbin.org/html';
    console.log(`Testing with: ${testUrl}`);
    
    const singleResult = await scraper.scrapePage(testUrl);
    
    if (singleResult.success) {
      console.log('âœ… Single page scraping successful!');
      console.log(`   ðŸ“„ Content length: ${singleResult.contentLength} characters`);
      console.log(`   ðŸ“ Text length: ${singleResult.textLength} characters`);
      console.log(`   ðŸ“Š Status: ${singleResult.status}`);
      console.log(`   ðŸ†” Record ID: ${singleResult.data.id}`);
    } else {
      console.log('âŒ Single page scraping failed:', singleResult.error);
    }

    // Test 2: Multiple pages scraping
    console.log('\n2ï¸âƒ£ Testing multiple pages scraping...');
    
    const testUrls = [
      'https://httpbin.org/html',
      'https://httpbin.org/json',
      'https://httpbin.org/xml'
    ];
    
    console.log(`Testing with ${testUrls.length} URLs...`);
    
    const multipleResults = await scraper.scrapeMultiplePages(testUrls);
    
    console.log('\nðŸ“Š Bulk scraping results:');
    console.log(`   âœ… Successful: ${multipleResults.successful.length}`);
    console.log(`   âŒ Failed: ${multipleResults.failed.length}`);
    console.log(`   ðŸ“ˆ Success rate: ${multipleResults.successRate}%`);
    console.log(`   â±ï¸  Duration: ${Math.round(multipleResults.duration / 1000)}s`);

    // Test 3: Retrieve crawled data
    console.log('\n3ï¸âƒ£ Testing data retrieval...');
    
    try {
      const crawledData = await scraper.getCrawledPage(testUrl);
      console.log('âœ… Data retrieval successful!');
      console.log(`   ðŸ”— URL: ${crawledData.url}`);
      console.log(`   ðŸ“… Fetched at: ${crawledData.fetched_at}`);
      console.log(`   ðŸ“‹ Metadata keys: ${Object.keys(crawledData.meta || {}).join(', ')}`);
      console.log(`   ðŸ“¦ Data keys: ${Object.keys(crawledData.data || {}).join(', ')}`);
    } catch (error) {
      console.log('âŒ Data retrieval failed:', error.message);
    }

    // Test 4: Search functionality
    console.log('\n4ï¸âƒ£ Testing search functionality...');
    
    try {
      const searchResults = await scraper.searchCrawledPages('html', { limit: 5 });
      console.log('âœ… Search successful!');
      console.log(`   ðŸ“Š Found ${searchResults.length} results`);
      searchResults.forEach((result, index) => {
        console.log(`   ${index + 1}. ${result.url} (${result.status})`);
      });
    } catch (error) {
      console.log('âŒ Search failed:', error.message);
    }

    // Test 5: Get crawling statistics
    console.log('\n5ï¸âƒ£ Testing statistics...');
    
    try {
      const stats = await scraper.getCrawlingStats();
      console.log('âœ… Statistics retrieved!');
      console.log(`   ðŸ“Š Total pages: ${stats.total}`);
      console.log(`   âœ… Successful: ${stats.successful}`);
      console.log(`   âŒ Failed: ${stats.failed}`);
      console.log(`   ðŸ“ˆ Success rate: ${stats.successRate}%`);
      console.log(`   ðŸ• Last 24h: ${stats.last24Hours}`);
      console.log(`   ðŸ“‹ Status codes:`, stats.statusCodes);
    } catch (error) {
      console.log('âŒ Statistics failed:', error.message);
    }

    // Test 6: Broker-specific scraping example
    console.log('\n6ï¸âƒ£ Testing broker-specific scraping...');
    
    // Example broker URLs (these are examples - replace with actual broker sites)
    const brokerUrls = [
      'https://example.com/broker1', // Replace with actual broker URLs
      'https://example.com/broker2'
    ];
    
    console.log('â„¹ï¸  Broker URL testing skipped (replace with actual broker URLs)');
    console.log('   Example usage:');
    console.log('   const brokerResults = await scraper.scrapeMultiplePages(brokerUrls);');

    console.log('\nðŸŽ‰ All tests completed!');
    console.log('\nðŸ“‹ Web Scraping Service is ready for use!');
    console.log('\nðŸ”§ Next steps:');
    console.log('   1. Replace test URLs with actual broker websites');
    console.log('   2. Configure Bright Data zone settings');
    console.log('   3. Set up scheduled crawling jobs');
    console.log('   4. Implement data analysis and insights');

  } catch (error) {
    console.error('âŒ Test failed with error:', error.message);
    console.error('Stack trace:', error.stack);
  }
}

// Helper function to demonstrate broker data extraction
function demonstrateBrokerDataExtraction() {
  console.log('\nðŸ“– Broker Data Extraction Examples:');
  console.log('\nðŸ¦ The service automatically extracts:');
  console.log('   â€¢ Regulation information (licenses, authorities)');
  console.log('   â€¢ Trading conditions (spreads, leverage, commissions)');
  console.log('   â€¢ Platform information (MT4, MT5, web platforms)');
  console.log('   â€¢ Available instruments (forex, CFDs, stocks)');
  console.log('   â€¢ Contact details (emails, phones, social media)');
  console.log('   â€¢ Ratings and reviews');
  console.log('   â€¢ Structured data (JSON-LD, microdata)');
  
  console.log('\nðŸ’¾ All data is stored in Supabase with:');
  console.log('   â€¢ Raw HTML content');
  console.log('   â€¢ Cleaned text content');
  console.log('   â€¢ Extracted metadata');
  console.log('   â€¢ Structured broker information');
  console.log('   â€¢ SHA256 hash for deduplication');
  console.log('   â€¢ Timestamp for tracking');
}

// Run the test
if (import.meta.url === `file://${process.argv[1]}`) {
  testWebScraping()
    .then(() => {
      demonstrateBrokerDataExtraction();
      process.exit(0);
    })
    .catch(error => {
      console.error('Test execution failed:', error);
      process.exit(1);
    });
}

export { testWebScraping, demonstrateBrokerDataExtraction };